{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "907f3d45",
   "metadata": {},
   "source": [
    "# Advanced Analytics: Machine Learning & User Segmentation\n",
    "\n",
    "## Objective\n",
    "Implement advanced analytics including skip prediction modeling, user segmentation through clustering, and retention analysis to drive data-driven product decisions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bfabaf1",
   "metadata": {},
   "source": [
    "## 1. Setup and Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "738977cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Machine Learning imports\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv('../data/spotify_cleaned.csv')\n",
    "session_stats = pd.read_csv('../data/session_statistics.csv')\n",
    "\n",
    "# Convert timestamp\n",
    "df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "\n",
    "print(f\"Loaded {len(df):,} records for advanced analytics\")\n",
    "print(f\"Target variable (skip rate): {df['is_skip'].mean():.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfd52c2d",
   "metadata": {},
   "source": [
    "## 2. Skip Prediction Model Development"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1df10ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature engineering for skip prediction\n",
    "def create_skip_features(df):\n",
    "    features_df = df.copy()\n",
    "    \n",
    "    # Temporal features\n",
    "    features_df['is_weekend'] = features_df['day_of_week_num'].isin([5, 6]).astype(int)\n",
    "    features_df['is_peak_hour'] = features_df['hour_of_day'].isin([17, 18, 19, 20]).astype(int)\n",
    "    features_df['is_night'] = features_df['hour_of_day'].isin([22, 23, 0, 1, 2, 3, 4, 5]).astype(int)\n",
    "    \n",
    "    # Track features\n",
    "    features_df['track_length_seconds'] = features_df['estimated_track_length_ms'] / 1000\n",
    "    features_df['is_long_track'] = (features_df['track_length_seconds'] > 240).astype(int)\n",
    "    features_df['is_short_track'] = (features_df['track_length_seconds'] < 120).astype(int)\n",
    "    \n",
    "    # Behavioral features\n",
    "    features_df['shuffle_int'] = features_df['shuffle'].astype(int)\n",
    "    \n",
    "    # Platform encoding\n",
    "    platform_dummies = pd.get_dummies(features_df['platform'], prefix='platform')\n",
    "    features_df = pd.concat([features_df, platform_dummies], axis=1)\n",
    "    \n",
    "    # Reason start encoding (top reasons only)\n",
    "    top_reasons = features_df['reason_start'].value_counts().head(5).index\n",
    "    for reason in top_reasons:\n",
    "        features_df[f'reason_start_{reason}'] = (features_df['reason_start'] == reason).astype(int)\n",
    "    \n",
    "    return features_df\n",
    "\n",
    "# Create features\n",
    "ml_df = create_skip_features(df)\n",
    "\n",
    "# Select features for modeling\n",
    "feature_columns = [\n",
    "    'hour_of_day', 'day_of_week_num', 'is_weekend', 'is_peak_hour', 'is_night',\n",
    "    'track_length_seconds', 'is_long_track', 'is_short_track', 'shuffle_int',\n",
    "    'platform_Android', 'platform_iOS', 'platform_web player',\n",
    "    'reason_start_clickrow', 'reason_start_trackdone', 'reason_start_autoplay'\n",
    "]\n",
    "\n",
    "# Filter features that exist\n",
    "available_features = [col for col in feature_columns if col in ml_df.columns]\n",
    "print(f\"Using {len(available_features)} features for modeling: {available_features}\")\n",
    "\n",
    "X = ml_df[available_features].fillna(0)\n",
    "y = ml_df['is_skip'].astype(int)\n",
    "\n",
    "print(f\"\\nFeature matrix shape: {X.shape}\")\n",
    "print(f\"Target distribution: Skip={y.sum():,} ({y.mean():.1%}), No Skip={len(y)-y.sum():,} ({1-y.mean():.1%})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eef3d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data and train models\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Train multiple models\n",
    "models = {\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42, max_depth=10),\n",
    "    'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(n_estimators=100, random_state=42, max_depth=6)\n",
    "}\n",
    "\n",
    "model_results = {}\n",
    "\n",
    "print(\"🤖 SKIP PREDICTION MODEL TRAINING\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "for name, model in models.items():\n",
    "    # Train model\n",
    "    if name == 'Logistic Regression':\n",
    "        model.fit(X_train_scaled, y_train)\n",
    "        y_pred = model.predict(X_test_scaled)\n",
    "        y_pred_proba = model.predict_proba(X_test_scaled)[:, 1]\n",
    "    else:\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = (y_pred == y_test).mean()\n",
    "    auc_score = roc_auc_score(y_test, y_pred_proba)\n",
    "    \n",
    "    model_results[name] = {\n",
    "        'model': model,\n",
    "        'accuracy': accuracy,\n",
    "        'auc': auc_score,\n",
    "        'predictions': y_pred,\n",
    "        'probabilities': y_pred_proba\n",
    "    }\n",
    "    \n",
    "    print(f\"{name}:\")\n",
    "    print(f\"  Accuracy: {accuracy:.3f}\")\n",
    "    print(f\"  AUC Score: {auc_score:.3f}\")\n",
    "    print()\n",
    "\n",
    "# Select best model\n",
    "best_model_name = max(model_results.keys(), key=lambda k: model_results[k]['auc'])\n",
    "best_model = model_results[best_model_name]\n",
    "print(f\"🏆 Best model: {best_model_name} (AUC: {best_model['auc']:.3f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da756abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed evaluation of best model\n",
    "print(f\"📊 DETAILED EVALUATION: {best_model_name}\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "# Classification report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, best_model['predictions']))\n",
    "\n",
    "# Feature importance (if available)\n",
    "if hasattr(best_model['model'], 'feature_importances_'):\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'feature': available_features,\n",
    "        'importance': best_model['model'].feature_importances_\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    print(f\"\\nTop 10 Feature Importances:\")\n",
    "    print(feature_importance.head(10))\n",
    "    \n",
    "    # Visualize feature importance\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.barplot(data=feature_importance.head(10), x='importance', y='feature')\n",
    "    plt.title(f'Top 10 Feature Importances - {best_model_name}')\n",
    "    plt.xlabel('Importance')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# ROC Curve\n",
    "plt.figure(figsize=(8, 6))\n",
    "fpr, tpr, _ = roc_curve(y_test, best_model['probabilities'])\n",
    "plt.plot(fpr, tpr, linewidth=2, label=f'{best_model_name} (AUC = {best_model[\"auc\"]:.3f})')\n",
    "plt.plot([0, 1], [0, 1], 'k--', alpha=0.5)\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve - Skip Prediction')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# Confusion Matrix\n",
    "plt.figure(figsize=(6, 5))\n",
    "cm = confusion_matrix(y_test, best_model['predictions'])\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['No Skip', 'Skip'], yticklabels=['No Skip', 'Skip'])\n",
    "plt.title('Confusion Matrix')\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df6f4893",
   "metadata": {},
   "source": [
    "## 3. User Segmentation through Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4013520f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create user-level features for clustering\n",
    "user_features = df.groupby('spotify_track_uri').agg({\n",
    "    'seconds_played': ['count', 'sum', 'mean'],\n",
    "    'is_skip': 'mean',\n",
    "    'percent_played': 'mean',\n",
    "    'shuffle': 'mean',\n",
    "    'hour_of_day': lambda x: x.mode().iloc[0] if len(x.mode()) > 0 else x.mean(),\n",
    "    'platform': lambda x: x.mode().iloc[0] if len(x.mode()) > 0 else x.iloc[0],\n",
    "    'day_of_week_num': 'mean'\n",
    "}).round(3)\n",
    "\n",
    "user_features.columns = ['play_count', 'total_seconds', 'avg_seconds_per_play', \n",
    "                        'skip_rate', 'avg_percent_played', 'shuffle_rate',\n",
    "                        'preferred_hour', 'preferred_platform', 'avg_day_of_week']\n",
    "\n",
    "# Add more behavioral features\n",
    "user_features['total_minutes'] = user_features['total_seconds'] / 60\n",
    "user_features['engagement_score'] = (\n",
    "    user_features['avg_percent_played'] * (1 - user_features['skip_rate'])\n",
    ")\n",
    "\n",
    "# Filter for tracks with sufficient data\n",
    "user_features = user_features[user_features['play_count'] >= 3]\n",
    "\n",
    "print(f\"Created user features for {len(user_features):,} tracks/users\")\n",
    "print(f\"\\nUser features summary:\")\n",
    "print(user_features.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cecd8d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare clustering features\n",
    "clustering_features = [\n",
    "    'play_count', 'avg_seconds_per_play', 'skip_rate', \n",
    "    'avg_percent_played', 'shuffle_rate', 'engagement_score'\n",
    "]\n",
    "\n",
    "X_cluster = user_features[clustering_features].fillna(0)\n",
    "\n",
    "# Scale features for clustering\n",
    "cluster_scaler = StandardScaler()\n",
    "X_cluster_scaled = cluster_scaler.fit_transform(X_cluster)\n",
    "\n",
    "# Find optimal number of clusters\n",
    "k_range = range(2, 10)\n",
    "inertias = []\n",
    "silhouette_scores = []\n",
    "\n",
    "for k in k_range:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "    cluster_labels = kmeans.fit_predict(X_cluster_scaled)\n",
    "    inertias.append(kmeans.inertia_)\n",
    "    silhouette_scores.append(silhouette_score(X_cluster_scaled, cluster_labels))\n",
    "\n",
    "# Plot elbow curve and silhouette scores\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "ax1.plot(k_range, inertias, marker='o')\n",
    "ax1.set_xlabel('Number of Clusters (k)')\n",
    "ax1.set_ylabel('Inertia')\n",
    "ax1.set_title('Elbow Method for Optimal k')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "ax2.plot(k_range, silhouette_scores, marker='o', color='orange')\n",
    "ax2.set_xlabel('Number of Clusters (k)')\n",
    "ax2.set_ylabel('Silhouette Score')\n",
    "ax2.set_title('Silhouette Score for Different k')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Select optimal k (highest silhouette score)\n",
    "optimal_k = k_range[np.argmax(silhouette_scores)]\n",
    "print(f\"Optimal number of clusters: {optimal_k} (Silhouette Score: {max(silhouette_scores):.3f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab9549ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform final clustering\n",
    "final_kmeans = KMeans(n_clusters=optimal_k, random_state=42, n_init=10)\n",
    "user_features['cluster'] = final_kmeans.fit_predict(X_cluster_scaled)\n",
    "\n",
    "# Analyze clusters\n",
    "cluster_analysis = user_features.groupby('cluster').agg({\n",
    "    'play_count': ['count', 'mean'],\n",
    "    'avg_seconds_per_play': 'mean',\n",
    "    'skip_rate': 'mean',\n",
    "    'avg_percent_played': 'mean',\n",
    "    'shuffle_rate': 'mean',\n",
    "    'engagement_score': 'mean',\n",
    "    'total_minutes': 'mean'\n",
    "}).round(3)\n",
    "\n",
    "cluster_analysis.columns = ['cluster_size', 'avg_play_count', 'avg_seconds_per_play',\n",
    "                           'avg_skip_rate', 'avg_percent_played', 'avg_shuffle_rate',\n",
    "                           'avg_engagement_score', 'avg_total_minutes']\n",
    "\n",
    "print(\"🎯 USER SEGMENTATION ANALYSIS\")\n",
    "print(\"=\" * 35)\n",
    "print(cluster_analysis)\n",
    "\n",
    "# Create cluster profiles\n",
    "cluster_profiles = {\n",
    "    0: \"Casual Listeners\",\n",
    "    1: \"Engaged Users\",\n",
    "    2: \"Power Users\",\n",
    "    3: \"Exploratory Users\",\n",
    "    4: \"Skip-Heavy Users\"\n",
    "}\n",
    "\n",
    "# Assign profiles based on characteristics\n",
    "print(f\"\\n👥 CLUSTER PROFILES:\")\n",
    "for cluster_id in range(optimal_k):\n",
    "    stats = cluster_analysis.loc[cluster_id]\n",
    "    size = stats['cluster_size']\n",
    "    skip_rate = stats['avg_skip_rate']\n",
    "    engagement = stats['avg_engagement_score']\n",
    "    play_count = stats['avg_play_count']\n",
    "    \n",
    "    print(f\"\\nCluster {cluster_id} ({size:,} users, {size/len(user_features)*100:.1f}%):\")\n",
    "    print(f\"  Average plays: {play_count:.1f}\")\n",
    "    print(f\"  Skip rate: {skip_rate:.1%}\")\n",
    "    print(f\"  Engagement score: {engagement:.2f}\")\n",
    "    print(f\"  Total listening: {stats['avg_total_minutes']:.1f} minutes\")\n",
    "    \n",
    "    # Characterize cluster\n",
    "    if skip_rate > 0.5:\n",
    "        profile = \"Skip-Heavy Users (Low Satisfaction)\"\n",
    "    elif engagement > 50 and play_count > 10:\n",
    "        profile = \"Power Users (High Value)\"\n",
    "    elif engagement > 30:\n",
    "        profile = \"Engaged Users (Core Audience)\"\n",
    "    else:\n",
    "        profile = \"Casual Listeners (Growth Opportunity)\"\n",
    "    \n",
    "    print(f\"  Profile: {profile}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac1a4d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize clusters\n",
    "# PCA for 2D visualization\n",
    "pca = PCA(n_components=2, random_state=42)\n",
    "X_pca = pca.fit_transform(X_cluster_scaled)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "scatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], c=user_features['cluster'], \n",
    "                     cmap='viridis', alpha=0.6, s=50)\n",
    "plt.colorbar(scatter)\n",
    "plt.xlabel(f'First Principal Component ({pca.explained_variance_ratio_[0]:.1%} variance)')\n",
    "plt.ylabel(f'Second Principal Component ({pca.explained_variance_ratio_[1]:.1%} variance)')\n",
    "plt.title('User Segmentation Clusters (PCA Visualization)')\n",
    "\n",
    "# Add cluster centers\n",
    "centers_pca = pca.transform(final_kmeans.cluster_centers_)\n",
    "plt.scatter(centers_pca[:, 0], centers_pca[:, 1], \n",
    "           c='red', marker='x', s=200, linewidths=3, label='Centroids')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# Feature importance in clusters\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for i, feature in enumerate(clustering_features):\n",
    "    cluster_feature_means = user_features.groupby('cluster')[feature].mean()\n",
    "    axes[i].bar(range(optimal_k), cluster_feature_means.values, \n",
    "               color=plt.cm.viridis(np.linspace(0, 1, optimal_k)))\n",
    "    axes[i].set_title(f'{feature.replace(\"_\", \" \").title()}')\n",
    "    axes[i].set_xlabel('Cluster')\n",
    "    axes[i].set_xticks(range(optimal_k))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e51d00e",
   "metadata": {},
   "source": [
    "## 4. Retention and Churn Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af00bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Daily activity analysis for retention\n",
    "df['date'] = df['timestamp'].dt.date\n",
    "daily_activity = df.groupby(['date']).agg({\n",
    "    'spotify_track_uri': 'count',\n",
    "    'seconds_played': 'sum',\n",
    "    'is_skip': 'mean'\n",
    "}).round(3)\n",
    "\n",
    "daily_activity.columns = ['daily_plays', 'daily_seconds', 'daily_skip_rate']\n",
    "daily_activity['daily_minutes'] = daily_activity['daily_seconds'] / 60\n",
    "\n",
    "# Convert date back to datetime for analysis\n",
    "daily_activity.reset_index(inplace=True)\n",
    "daily_activity['date'] = pd.to_datetime(daily_activity['date'])\n",
    "daily_activity = daily_activity.sort_values('date')\n",
    "\n",
    "# Calculate retention metrics\n",
    "total_days = (daily_activity['date'].max() - daily_activity['date'].min()).days + 1\n",
    "active_days = len(daily_activity)\n",
    "retention_rate = active_days / total_days\n",
    "\n",
    "print(\"📈 RETENTION & CHURN ANALYSIS\")\n",
    "print(\"=\" * 35)\n",
    "print(f\"Total period: {total_days} days\")\n",
    "print(f\"Active days: {active_days} days\")\n",
    "print(f\"Daily retention rate: {retention_rate:.1%}\")\n",
    "print(f\"Average daily plays: {daily_activity['daily_plays'].mean():.0f}\")\n",
    "print(f\"Average daily listening: {daily_activity['daily_minutes'].mean():.1f} minutes\")\n",
    "\n",
    "# Activity consistency (engagement stability)\n",
    "activity_cv = daily_activity['daily_plays'].std() / daily_activity['daily_plays'].mean()\n",
    "print(f\"Activity consistency (lower is better): {activity_cv:.2f}\")\n",
    "\n",
    "# Weekly retention patterns\n",
    "daily_activity['week'] = daily_activity['date'].dt.isocalendar().week\n",
    "weekly_activity = daily_activity.groupby('week').agg({\n",
    "    'daily_plays': ['count', 'sum', 'mean'],\n",
    "    'daily_minutes': ['sum', 'mean'],\n",
    "    'daily_skip_rate': 'mean'\n",
    "}).round(2)\n",
    "\n",
    "weekly_activity.columns = ['active_days_per_week', 'total_weekly_plays', 'avg_daily_plays',\n",
    "                          'total_weekly_minutes', 'avg_daily_minutes', 'avg_weekly_skip_rate']\n",
    "\n",
    "print(f\"\\nWeekly patterns:\")\n",
    "print(f\"Average active days per week: {weekly_activity['active_days_per_week'].mean():.1f}\")\n",
    "print(f\"Most active week: {weekly_activity['total_weekly_plays'].max():,.0f} plays\")\n",
    "print(f\"Least active week: {weekly_activity['total_weekly_plays'].min():,.0f} plays\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d520c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Churn risk identification\n",
    "# Define churn risk based on recent activity decline\n",
    "daily_activity = daily_activity.sort_values('date')\n",
    "daily_activity['rolling_7d_plays'] = daily_activity['daily_plays'].rolling(7, center=True).mean()\n",
    "daily_activity['rolling_7d_minutes'] = daily_activity['daily_minutes'].rolling(7, center=True).mean()\n",
    "\n",
    "# Activity trend (last 7 days vs previous 7 days)\n",
    "recent_activity = daily_activity.tail(7)['daily_plays'].mean()\n",
    "previous_activity = daily_activity.iloc[-14:-7]['daily_plays'].mean() if len(daily_activity) >= 14 else recent_activity\n",
    "\n",
    "activity_trend = (recent_activity - previous_activity) / previous_activity if previous_activity > 0 else 0\n",
    "\n",
    "print(f\"\\n🔄 ACTIVITY TRENDS:\")\n",
    "print(f\"Recent 7-day avg: {recent_activity:.1f} plays/day\")\n",
    "print(f\"Previous 7-day avg: {previous_activity:.1f} plays/day\")\n",
    "print(f\"Activity trend: {activity_trend:+.1%}\")\n",
    "\n",
    "if activity_trend < -0.2:\n",
    "    churn_risk = \"High\"\n",
    "elif activity_trend < -0.1:\n",
    "    churn_risk = \"Medium\"\n",
    "else:\n",
    "    churn_risk = \"Low\"\n",
    "\n",
    "print(f\"Churn risk level: {churn_risk}\")\n",
    "\n",
    "# Visualize retention trends\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(15, 10))\n",
    "\n",
    "# Daily activity trend\n",
    "ax1.plot(daily_activity['date'], daily_activity['daily_plays'], alpha=0.5, label='Daily')\n",
    "ax1.plot(daily_activity['date'], daily_activity['rolling_7d_plays'], \n",
    "         linewidth=2, color='red', label='7-day Average')\n",
    "ax1.set_title('Daily Activity Trend')\n",
    "ax1.set_ylabel('Plays per Day')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Weekly activity pattern\n",
    "ax2.bar(weekly_activity.index, weekly_activity['total_weekly_plays'], \n",
    "        color='skyblue', alpha=0.7)\n",
    "ax2.set_title('Weekly Activity Pattern')\n",
    "ax2.set_xlabel('Week Number')\n",
    "ax2.set_ylabel('Total Weekly Plays')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6450dfbd",
   "metadata": {},
   "source": [
    "## 5. Advanced Business Insights & Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0b3b8c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Synthesize insights from ML models\n",
    "print(\"🧠 ADVANCED ANALYTICS INSIGHTS\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Skip prediction insights\n",
    "if hasattr(best_model['model'], 'feature_importances_'):\n",
    "    top_skip_drivers = feature_importance.head(3)\n",
    "    print(f\"\\n🎯 SKIP PREDICTION INSIGHTS:\")\n",
    "    print(f\"• Model accuracy: {best_model['accuracy']:.1%}\")\n",
    "    print(f\"• Top skip drivers:\")\n",
    "    for _, row in top_skip_drivers.iterrows():\n",
    "        print(f\"  - {row['feature']}: {row['importance']:.3f} importance\")\n",
    "\n",
    "# User segmentation insights\n",
    "print(f\"\\n👥 USER SEGMENTATION INSIGHTS:\")\n",
    "high_value_clusters = cluster_analysis[cluster_analysis['avg_engagement_score'] > \n",
    "                                     cluster_analysis['avg_engagement_score'].mean()]\n",
    "risk_clusters = cluster_analysis[cluster_analysis['avg_skip_rate'] > 0.5]\n",
    "\n",
    "print(f\"• Identified {optimal_k} distinct user segments\")\n",
    "print(f\"• {len(high_value_clusters)} high-engagement segments ({len(high_value_clusters)/optimal_k*100:.0f}%)\")\n",
    "if len(risk_clusters) > 0:\n",
    "    print(f\"• {len(risk_clusters)} at-risk segments with >50% skip rate\")\n",
    "\n",
    "# Retention insights\n",
    "print(f\"\\n📈 RETENTION INSIGHTS:\")\n",
    "print(f\"• Daily retention rate: {retention_rate:.1%}\")\n",
    "print(f\"• Activity trend: {activity_trend:+.1%}\")\n",
    "print(f\"• Churn risk level: {churn_risk}\")\n",
    "\n",
    "# Action recommendations\n",
    "print(f\"\\n🎯 ML-DRIVEN RECOMMENDATIONS:\")\n",
    "\n",
    "recommendations = []\n",
    "\n",
    "# Skip prediction recommendations\n",
    "if 'track_length_seconds' in feature_importance.head(3)['feature'].values:\n",
    "    recommendations.append(\"Optimize track length recommendations - length is a key skip driver\")\n",
    "\n",
    "if 'shuffle_int' in feature_importance.head(3)['feature'].values:\n",
    "    recommendations.append(\"Improve shuffle algorithm - shuffle mode affects skip behavior\")\n",
    "\n",
    "# User segmentation recommendations\n",
    "total_users = cluster_analysis['cluster_size'].sum()\n",
    "casual_users_pct = (cluster_analysis[cluster_analysis['avg_engagement_score'] < 30]['cluster_size'].sum() / total_users) * 100\n",
    "\n",
    "if casual_users_pct > 40:\n",
    "    recommendations.append(f\"Focus on converting {casual_users_pct:.0f}% casual users to engaged users\")\n",
    "\n",
    "if len(risk_clusters) > 0:\n",
    "    risk_users_pct = (risk_clusters['cluster_size'].sum() / total_users) * 100\n",
    "    recommendations.append(f\"Implement retention strategy for {risk_users_pct:.0f}% high-skip users\")\n",
    "\n",
    "# Retention recommendations\n",
    "if activity_trend < -0.1:\n",
    "    recommendations.append(\"Implement re-engagement campaign - activity is declining\")\n",
    "\n",
    "if retention_rate < 0.7:\n",
    "    recommendations.append(\"Improve daily retention through personalization and notifications\")\n",
    "\n",
    "# Platform-specific recommendations\n",
    "platform_performance = df.groupby('platform')['is_skip'].mean()\n",
    "worst_platform = platform_performance.idxmax()\n",
    "recommendations.append(f\"Optimize {worst_platform} experience - highest skip rate platform\")\n",
    "\n",
    "for i, rec in enumerate(recommendations, 1):\n",
    "    print(f\"{i}. {rec}\")\n",
    "\n",
    "# Save ML results\n",
    "user_features.to_csv('../data/user_segments.csv')\n",
    "daily_activity.to_csv('../data/retention_analysis.csv', index=False)\n",
    "\n",
    "print(f\"\\n✅ Advanced analytics results saved to ../data/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85ad44d8",
   "metadata": {},
   "source": [
    "## 6. Model Performance Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4467f7b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive model summary\n",
    "model_summary = pd.DataFrame({\n",
    "    'Model': list(model_results.keys()),\n",
    "    'Accuracy': [results['accuracy'] for results in model_results.values()],\n",
    "    'AUC Score': [results['auc'] for results in model_results.values()]\n",
    "})\n",
    "\n",
    "print(\"🤖 MODEL PERFORMANCE SUMMARY\")\n",
    "print(\"=\" * 35)\n",
    "print(model_summary.round(3))\n",
    "\n",
    "# Clustering summary\n",
    "clustering_summary = {\n",
    "    'Optimal Clusters': optimal_k,\n",
    "    'Silhouette Score': max(silhouette_scores),\n",
    "    'Total Users Segmented': len(user_features),\n",
    "    'Variance Explained (PCA)': sum(pca.explained_variance_ratio_)\n",
    "}\n",
    "\n",
    "print(f\"\\n🎯 CLUSTERING SUMMARY:\")\n",
    "for metric, value in clustering_summary.items():\n",
    "    if isinstance(value, float):\n",
    "        print(f\"{metric}: {value:.3f}\")\n",
    "    else:\n",
    "        print(f\"{metric}: {value:,}\")\n",
    "\n",
    "# Export final summary\n",
    "model_summary.to_csv('../reports/model_performance.csv', index=False)\n",
    "\n",
    "final_summary = {\n",
    "    'Best Model': best_model_name,\n",
    "    'Best Model AUC': best_model['auc'],\n",
    "    'User Segments': optimal_k,\n",
    "    'Retention Rate': retention_rate,\n",
    "    'Activity Trend': activity_trend,\n",
    "    'Churn Risk': churn_risk\n",
    "}\n",
    "\n",
    "summary_df = pd.DataFrame(list(final_summary.items()), columns=['Metric', 'Value'])\n",
    "summary_df.to_csv('../reports/advanced_analytics_summary.csv', index=False)\n",
    "\n",
    "print(f\"\\n📊 FINAL ANALYTICS SUMMARY:\")\n",
    "print(summary_df.to_string(index=False))\n",
    "print(f\"\\n✅ All advanced analytics reports saved to ../reports/\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
