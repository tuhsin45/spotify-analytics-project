{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "21a130d5",
   "metadata": {},
   "source": [
    "# Spotify Business Intelligence & Product Insights\n",
    "\n",
    "## Objective\n",
    "Extract actionable business insights from Spotify streaming data to inform product decisions, user engagement strategies, and platform optimization efforts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bd8546c",
   "metadata": {},
   "source": [
    "## 1. Setup and Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b5ab04f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Load cleaned data\n",
    "df = pd.read_csv('../data/spotify_cleaned.csv')\n",
    "session_stats = pd.read_csv('../data/session_statistics.csv')\n",
    "\n",
    "# Convert timestamp back to datetime\n",
    "df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "\n",
    "print(f\"Loaded {len(df):,} streaming records\")\n",
    "print(f\"Loaded {len(session_stats):,} session records\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28c4ee15",
   "metadata": {},
   "source": [
    "## 2. Peak Usage Analysis & Platform Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e03e0d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Peak usage times analysis for resource planning\n",
    "hourly_usage = df.groupby('hour_of_day').agg({\n",
    "    'spotify_track_uri': 'count',\n",
    "    'seconds_played': 'sum',\n",
    "    'is_skip': 'mean'\n",
    "}).round(3)\n",
    "\n",
    "hourly_usage.columns = ['Total_Plays', 'Total_Seconds', 'Skip_Rate']\n",
    "hourly_usage['Total_Hours'] = hourly_usage['Total_Seconds'] / 3600\n",
    "\n",
    "# Create interactive peak usage visualization\n",
    "fig = make_subplots(\n",
    "    rows=2, cols=2,\n",
    "    subplot_titles=('Hourly Play Volume', 'Hourly Listening Time', \n",
    "                   'Platform Usage by Hour', 'Skip Rate by Hour'),\n",
    "    specs=[[{\"secondary_y\": False}, {\"secondary_y\": False}],\n",
    "           [{\"secondary_y\": False}, {\"secondary_y\": False}]]\n",
    ")\n",
    "\n",
    "# Hourly plays\n",
    "fig.add_trace(\n",
    "    go.Bar(x=hourly_usage.index, y=hourly_usage['Total_Plays'], \n",
    "           name='Total Plays', marker_color='skyblue'),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "# Hourly listening time\n",
    "fig.add_trace(\n",
    "    go.Bar(x=hourly_usage.index, y=hourly_usage['Total_Hours'], \n",
    "           name='Total Hours', marker_color='lightcoral'),\n",
    "    row=1, col=2\n",
    ")\n",
    "\n",
    "# Platform usage by hour (stacked bar)\n",
    "platform_hourly = df.groupby(['hour_of_day', 'platform']).size().unstack(fill_value=0)\n",
    "for platform in platform_hourly.columns:\n",
    "    fig.add_trace(\n",
    "        go.Bar(x=platform_hourly.index, y=platform_hourly[platform], \n",
    "               name=platform),\n",
    "        row=2, col=1\n",
    "    )\n",
    "\n",
    "# Skip rate by hour\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=hourly_usage.index, y=hourly_usage['Skip_Rate']*100, \n",
    "              mode='lines+markers', name='Skip Rate %', \n",
    "              line=dict(color='orange', width=3)),\n",
    "    row=2, col=2\n",
    ")\n",
    "\n",
    "fig.update_layout(height=700, title_text=\"Peak Usage Analysis Dashboard\")\n",
    "fig.show()\n",
    "\n",
    "# Business insights\n",
    "peak_hour = hourly_usage['Total_Plays'].idxmax()\n",
    "low_hour = hourly_usage['Total_Plays'].idxmin()\n",
    "peak_skip_hour = hourly_usage['Skip_Rate'].idxmax()\n",
    "\n",
    "print(f\"ðŸ“Š PEAK USAGE INSIGHTS:\")\n",
    "print(f\"Peak usage hour: {peak_hour}:00 ({hourly_usage.loc[peak_hour, 'Total_Plays']:,} plays)\")\n",
    "print(f\"Lowest usage hour: {low_hour}:00 ({hourly_usage.loc[low_hour, 'Total_Plays']:,} plays)\")\n",
    "print(f\"Highest skip rate: {peak_skip_hour}:00 ({hourly_usage.loc[peak_skip_hour, 'Skip_Rate']:.1%})\")\n",
    "print(f\"Peak usage is {hourly_usage.loc[peak_hour, 'Total_Plays'] / hourly_usage.loc[low_hour, 'Total_Plays']:.1f}x higher than lowest\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c495c01c",
   "metadata": {},
   "source": [
    "## 3. Platform Performance & User Experience Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a319b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive platform analysis\n",
    "platform_metrics = df.groupby('platform').agg({\n",
    "    'spotify_track_uri': 'count',\n",
    "    'seconds_played': ['sum', 'mean'],\n",
    "    'is_skip': 'mean',\n",
    "    'percent_played': 'mean',\n",
    "    'shuffle': 'mean'\n",
    "}).round(3)\n",
    "\n",
    "platform_metrics.columns = ['Total_Plays', 'Total_Seconds', 'Avg_Seconds_Per_Play', \n",
    "                            'Skip_Rate', 'Avg_Percent_Played', 'Shuffle_Usage_Rate']\n",
    "\n",
    "platform_metrics['Total_Hours'] = platform_metrics['Total_Seconds'] / 3600\n",
    "platform_metrics['Market_Share'] = (platform_metrics['Total_Plays'] / platform_metrics['Total_Plays'].sum() * 100).round(1)\n",
    "\n",
    "print(\"ðŸŽ¯ PLATFORM PERFORMANCE DASHBOARD\")\n",
    "print(\"=\" * 50)\n",
    "print(platform_metrics)\n",
    "\n",
    "# Platform engagement quality score\n",
    "# Higher percent played + lower skip rate + longer sessions = better engagement\n",
    "platform_metrics['Engagement_Score'] = (\n",
    "    (platform_metrics['Avg_Percent_Played'] / platform_metrics['Avg_Percent_Played'].max()) * 0.4 +\n",
    "    ((1 - platform_metrics['Skip_Rate']) / (1 - platform_metrics['Skip_Rate']).max()) * 0.4 +\n",
    "    (platform_metrics['Avg_Seconds_Per_Play'] / platform_metrics['Avg_Seconds_Per_Play'].max()) * 0.2\n",
    ") * 100\n",
    "\n",
    "print(f\"\\nðŸ“± PLATFORM RANKINGS:\")\n",
    "platform_ranking = platform_metrics.sort_values('Engagement_Score', ascending=False)\n",
    "for i, (platform, score) in enumerate(platform_ranking['Engagement_Score'].items(), 1):\n",
    "    print(f\"{i}. {platform}: {score:.1f} engagement score\")\n",
    "\n",
    "# Platform switching analysis\n",
    "platform_switches = df.sort_values('timestamp')\n",
    "platform_switches['prev_platform'] = platform_switches['platform'].shift(1)\n",
    "platform_switches['platform_switch'] = platform_switches['platform'] != platform_switches['prev_platform']\n",
    "switch_rate = platform_switches['platform_switch'].mean()\n",
    "\n",
    "print(f\"\\nðŸ”„ Platform switch rate: {switch_rate:.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "507132cd",
   "metadata": {},
   "source": [
    "## 4. Skip Behavior Deep Dive & Content Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fefea86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced skip analysis for content strategy\n",
    "# Skip drivers analysis\n",
    "skip_drivers = {\n",
    "    'Platform': df.groupby('platform')['is_skip'].mean(),\n",
    "    'Time of Day': df.groupby('time_of_day')['is_skip'].mean(),\n",
    "    'Shuffle Mode': df.groupby('shuffle')['is_skip'].mean(),\n",
    "    'Day of Week': df.groupby('day_of_week')['is_skip'].mean()\n",
    "}\n",
    "\n",
    "print(\"ðŸŽµ SKIP BEHAVIOR ANALYSIS\")\n",
    "print(\"=\" * 40)\n",
    "for category, data in skip_drivers.items():\n",
    "    print(f\"\\n{category}:\")\n",
    "    for item, rate in data.sort_values(ascending=False).items():\n",
    "        print(f\"  {item}: {rate:.1%}\")\n",
    "\n",
    "# Track length vs skip analysis\n",
    "df['track_length_category'] = pd.cut(df['estimated_track_length_ms']/1000, \n",
    "                                    bins=[0, 120, 180, 240, 300, float('inf')],\n",
    "                                    labels=['<2min', '2-3min', '3-4min', '4-5min', '5min+'])\n",
    "\n",
    "length_skip_analysis = df.groupby('track_length_category').agg({\n",
    "    'is_skip': ['mean', 'count'],\n",
    "    'percent_played': 'mean'\n",
    "}).round(3)\n",
    "\n",
    "print(f\"\\nðŸŽµ TRACK LENGTH vs SKIP BEHAVIOR:\")\n",
    "print(length_skip_analysis)\n",
    "\n",
    "# Create skip prediction features importance\n",
    "skip_features = df[['hour_of_day', 'day_of_week_num', 'estimated_track_length_ms', \n",
    "                   'shuffle', 'platform']].copy()\n",
    "skip_features = pd.get_dummies(skip_features, columns=['platform'])\n",
    "skip_features['shuffle'] = skip_features['shuffle'].astype(int)\n",
    "\n",
    "# Correlation with skip behavior\n",
    "correlations = skip_features.corrwith(df['is_skip'].astype(int)).sort_values(key=abs, ascending=False)\n",
    "print(f\"\\nðŸ” SKIP PREDICTION FEATURE IMPORTANCE:\")\n",
    "for feature, corr in correlations.items():\n",
    "    if abs(corr) > 0.01:  # Only show meaningful correlations\n",
    "        print(f\"  {feature}: {corr:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "611918fa",
   "metadata": {},
   "source": [
    "## 5. User Engagement & Retention Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09837252",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Daily and weekly activity analysis (retention proxy)\n",
    "df['date'] = df['timestamp'].dt.date\n",
    "daily_activity = df.groupby('date').agg({\n",
    "    'spotify_track_uri': 'count',\n",
    "    'seconds_played': 'sum',\n",
    "    'is_skip': 'mean'\n",
    "}).round(3)\n",
    "\n",
    "daily_activity.columns = ['Daily_Plays', 'Daily_Seconds', 'Daily_Skip_Rate']\n",
    "daily_activity['Daily_Hours'] = daily_activity['Daily_Seconds'] / 3600\n",
    "\n",
    "# Weekly patterns\n",
    "df['week'] = df['timestamp'].dt.isocalendar().week\n",
    "weekly_activity = df.groupby('week').agg({\n",
    "    'spotify_track_uri': 'count',\n",
    "    'seconds_played': 'sum',\n",
    "    'date': 'nunique'  # Active days per week\n",
    "}).round(3)\n",
    "\n",
    "weekly_activity.columns = ['Weekly_Plays', 'Weekly_Seconds', 'Active_Days']\n",
    "weekly_activity['Weekly_Hours'] = weekly_activity['Weekly_Seconds'] / 3600\n",
    "\n",
    "print(\"ðŸ“ˆ USER ACTIVITY & RETENTION METRICS\")\n",
    "print(\"=\" * 45)\n",
    "print(f\"Average daily plays: {daily_activity['Daily_Plays'].mean():.0f}\")\n",
    "print(f\"Average daily listening: {daily_activity['Daily_Hours'].mean():.1f} hours\")\n",
    "print(f\"Most active day: {daily_activity['Daily_Plays'].max():,.0f} plays\")\n",
    "print(f\"Least active day: {daily_activity['Daily_Plays'].min():,.0f} plays\")\n",
    "print(f\"Activity consistency (CV): {(daily_activity['Daily_Plays'].std() / daily_activity['Daily_Plays'].mean()):.2f}\")\n",
    "\n",
    "# Engagement trends over time\n",
    "daily_activity.reset_index(inplace=True)\n",
    "daily_activity['date'] = pd.to_datetime(daily_activity['date'])\n",
    "daily_activity = daily_activity.sort_values('date')\n",
    "\n",
    "# Calculate 7-day rolling averages\n",
    "daily_activity['Rolling_Plays'] = daily_activity['Daily_Plays'].rolling(7, center=True).mean()\n",
    "daily_activity['Rolling_Hours'] = daily_activity['Daily_Hours'].rolling(7, center=True).mean()\n",
    "\n",
    "# Visualize trends\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(15, 10))\n",
    "\n",
    "# Daily plays trend\n",
    "ax1.plot(daily_activity['date'], daily_activity['Daily_Plays'], alpha=0.3, label='Daily')\n",
    "ax1.plot(daily_activity['date'], daily_activity['Rolling_Plays'], color='red', linewidth=2, label='7-day Average')\n",
    "ax1.set_title('Daily Listening Activity Trend')\n",
    "ax1.set_ylabel('Number of Plays')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Daily hours trend\n",
    "ax2.plot(daily_activity['date'], daily_activity['Daily_Hours'], alpha=0.3, label='Daily')\n",
    "ax2.plot(daily_activity['date'], daily_activity['Rolling_Hours'], color='blue', linewidth=2, label='7-day Average')\n",
    "ax2.set_title('Daily Listening Time Trend')\n",
    "ax2.set_ylabel('Hours')\n",
    "ax2.set_xlabel('Date')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e95d6961",
   "metadata": {},
   "source": [
    "## 6. Session Quality & User Journey Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f33ad7b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Session quality scoring\n",
    "session_stats['Session_Quality_Score'] = (\n",
    "    (session_stats['Session_Duration_Minutes'] / session_stats['Session_Duration_Minutes'].max()) * 0.3 +\n",
    "    (session_stats['Tracks_Count'] / session_stats['Tracks_Count'].max()) * 0.3 +\n",
    "    ((1 - session_stats['Skip_Rate']) / (1 - session_stats['Skip_Rate']).max()) * 0.4\n",
    ") * 100\n",
    "\n",
    "# Session quality categories\n",
    "session_stats['Quality_Category'] = pd.cut(session_stats['Session_Quality_Score'],\n",
    "                                          bins=[0, 33, 66, 100],\n",
    "                                          labels=['Low', 'Medium', 'High'])\n",
    "\n",
    "quality_distribution = session_stats['Quality_Category'].value_counts(normalize=True) * 100\n",
    "\n",
    "print(\"ðŸ† SESSION QUALITY ANALYSIS\")\n",
    "print(\"=\" * 35)\n",
    "print(f\"High Quality Sessions: {quality_distribution.get('High', 0):.1f}%\")\n",
    "print(f\"Medium Quality Sessions: {quality_distribution.get('Medium', 0):.1f}%\")\n",
    "print(f\"Low Quality Sessions: {quality_distribution.get('Low', 0):.1f}%\")\n",
    "\n",
    "# Platform quality comparison\n",
    "platform_quality = session_stats.groupby('Primary_Platform')['Session_Quality_Score'].mean().sort_values(ascending=False)\n",
    "print(f\"\\nðŸ“± PLATFORM SESSION QUALITY:\")\n",
    "for platform, score in platform_quality.items():\n",
    "    print(f\"  {platform}: {score:.1f} avg quality score\")\n",
    "\n",
    "# User journey patterns\n",
    "# Analyze reason_start and reason_end patterns\n",
    "journey_analysis = df.groupby(['reason_start', 'reason_end']).agg({\n",
    "    'spotify_track_uri': 'count',\n",
    "    'is_skip': 'mean',\n",
    "    'percent_played': 'mean'\n",
    "}).round(3)\n",
    "\n",
    "journey_analysis.columns = ['Count', 'Skip_Rate', 'Avg_Percent_Played']\n",
    "journey_analysis = journey_analysis[journey_analysis['Count'] >= 100]  # Filter for common patterns\n",
    "journey_analysis = journey_analysis.sort_values('Count', ascending=False)\n",
    "\n",
    "print(f\"\\nðŸ›¤ï¸  TOP USER JOURNEY PATTERNS:\")\n",
    "print(journey_analysis.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cccd1078",
   "metadata": {},
   "source": [
    "## 7. Content Performance & Recommendation Insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e6c472",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Artist and track performance analysis\n",
    "artist_performance = df.groupby('artist_name').agg({\n",
    "    'spotify_track_uri': 'count',\n",
    "    'track_name': 'nunique',\n",
    "    'seconds_played': 'sum',\n",
    "    'is_skip': 'mean',\n",
    "    'percent_played': 'mean',\n",
    "    'shuffle': 'mean'\n",
    "}).round(3)\n",
    "\n",
    "artist_performance.columns = ['Total_Plays', 'Unique_Tracks', 'Total_Seconds', \n",
    "                             'Skip_Rate', 'Avg_Percent_Played', 'Shuffle_Rate']\n",
    "\n",
    "# Filter for artists with significant play count\n",
    "artist_performance = artist_performance[artist_performance['Total_Plays'] >= 20]\n",
    "artist_performance['Total_Hours'] = artist_performance['Total_Seconds'] / 3600\n",
    "\n",
    "# Artist engagement score\n",
    "artist_performance['Engagement_Score'] = (\n",
    "    (artist_performance['Avg_Percent_Played'] / artist_performance['Avg_Percent_Played'].max()) * 0.4 +\n",
    "    ((1 - artist_performance['Skip_Rate']) / (1 - artist_performance['Skip_Rate']).max()) * 0.4 +\n",
    "    (artist_performance['Total_Plays'] / artist_performance['Total_Plays'].max()) * 0.2\n",
    ") * 100\n",
    "\n",
    "# Top performers\n",
    "top_engaging_artists = artist_performance.sort_values('Engagement_Score', ascending=False).head(10)\n",
    "most_played_artists = artist_performance.sort_values('Total_Plays', ascending=False).head(10)\n",
    "least_skipped_artists = artist_performance.sort_values('Skip_Rate').head(10)\n",
    "\n",
    "print(\"ðŸŽ¤ ARTIST PERFORMANCE INSIGHTS\")\n",
    "print(\"=\" * 40)\n",
    "print(\"\\nTop 10 Most Engaging Artists:\")\n",
    "print(top_engaging_artists[['Total_Plays', 'Skip_Rate', 'Engagement_Score']].head())\n",
    "\n",
    "print(\"\\nTop 10 Least Skipped Artists:\")\n",
    "print(least_skipped_artists[['Total_Plays', 'Skip_Rate', 'Avg_Percent_Played']].head())\n",
    "\n",
    "# Genre insights (using reason_start as proxy for discovery vs intentional)\n",
    "discovery_analysis = df.groupby('reason_start').agg({\n",
    "    'spotify_track_uri': 'count',\n",
    "    'is_skip': 'mean',\n",
    "    'percent_played': 'mean'\n",
    "}).round(3)\n",
    "\n",
    "discovery_analysis.columns = ['Count', 'Skip_Rate', 'Avg_Percent_Played']\n",
    "discovery_analysis = discovery_analysis.sort_values('Count', ascending=False)\n",
    "\n",
    "print(f\"\\nðŸ” CONTENT DISCOVERY PATTERNS:\")\n",
    "print(discovery_analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "054a3ecd",
   "metadata": {},
   "source": [
    "## 8. Business Recommendations Dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d45341",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate actionable business recommendations\n",
    "print(\"ðŸŽ¯ BUSINESS RECOMMENDATIONS DASHBOARD\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# 1. Platform Optimization\n",
    "best_platform = platform_metrics['Engagement_Score'].idxmax()\n",
    "worst_platform = platform_metrics['Engagement_Score'].idxmin()\n",
    "print(f\"\\nðŸ“± PLATFORM STRATEGY:\")\n",
    "print(f\"âœ… Invest in {best_platform} (highest engagement: {platform_metrics.loc[best_platform, 'Engagement_Score']:.1f})\")\n",
    "print(f\"âš ï¸  Optimize {worst_platform} user experience (lowest engagement: {platform_metrics.loc[worst_platform, 'Engagement_Score']:.1f})\")\n",
    "\n",
    "# 2. Content Strategy\n",
    "high_skip_threshold = df['is_skip'].quantile(0.75)\n",
    "content_recommendations = []\n",
    "\n",
    "if df[df['shuffle'] == True]['is_skip'].mean() > df[df['shuffle'] == False]['is_skip'].mean():\n",
    "    content_recommendations.append(\"Improve shuffle algorithm - higher skip rates in shuffle mode\")\n",
    "    \n",
    "peak_skip_time = df.groupby('time_of_day')['is_skip'].mean().idxmax()\n",
    "content_recommendations.append(f\"Focus on {peak_skip_time.lower()} playlists - highest skip period\")\n",
    "\n",
    "print(f\"\\nðŸŽµ CONTENT STRATEGY:\")\n",
    "for i, rec in enumerate(content_recommendations, 1):\n",
    "    print(f\"{i}. {rec}\")\n",
    "\n",
    "# 3. User Engagement\n",
    "avg_session_quality = session_stats['Session_Quality_Score'].mean()\n",
    "low_quality_sessions_pct = (session_stats['Session_Quality_Score'] < 33).mean() * 100\n",
    "\n",
    "print(f\"\\nðŸ‘¥ USER ENGAGEMENT:\")\n",
    "print(f\"â€¢ Average session quality: {avg_session_quality:.1f}/100\")\n",
    "print(f\"â€¢ {low_quality_sessions_pct:.1f}% of sessions are low quality\")\n",
    "print(f\"â€¢ Target: Reduce low-quality sessions by improving onboarding\")\n",
    "\n",
    "# 4. Peak Usage Optimization\n",
    "peak_usage_ratio = hourly_usage['Total_Plays'].max() / hourly_usage['Total_Plays'].min()\n",
    "print(f\"\\nâ° INFRASTRUCTURE:\")\n",
    "print(f\"â€¢ Peak usage is {peak_usage_ratio:.1f}x higher than low periods\")\n",
    "print(f\"â€¢ Scale infrastructure for {peak_hour}:00 peak demand\")\n",
    "print(f\"â€¢ Consider off-peak promotions during {low_hour}:00-{(low_hour+2)%24}:00\")\n",
    "\n",
    "# 5. Key Performance Targets\n",
    "current_skip_rate = df['is_skip'].mean()\n",
    "target_skip_rate = current_skip_rate * 0.9  # 10% improvement\n",
    "current_avg_session = session_stats['Session_Duration_Minutes'].mean()\n",
    "target_session_length = current_avg_session * 1.15  # 15% improvement\n",
    "\n",
    "print(f\"\\nðŸŽ¯ PERFORMANCE TARGETS:\")\n",
    "print(f\"â€¢ Reduce skip rate from {current_skip_rate:.1%} to {target_skip_rate:.1%}\")\n",
    "print(f\"â€¢ Increase avg session from {current_avg_session:.1f} to {target_session_length:.1f} minutes\")\n",
    "print(f\"â€¢ Improve {worst_platform} engagement by 20%\")\n",
    "print(f\"â€¢ Increase high-quality sessions from {quality_distribution.get('High', 0):.1f}% to 40%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bff8fe8f",
   "metadata": {},
   "source": [
    "## 9. Executive Summary Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cdb7644",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create executive summary with key insights\n",
    "total_users_proxy = len(df)  # Using total plays as user proxy\n",
    "total_content_hours = df['seconds_played'].sum() / 3600\n",
    "revenue_proxy = total_content_hours * 0.004  # Rough estimate: $0.004 per stream hour\n",
    "\n",
    "executive_summary = {\n",
    "    'Total Streaming Events': f\"{len(df):,}\",\n",
    "    'Total Content Hours': f\"{total_content_hours:,.0f}\",\n",
    "    'Revenue Proxy ($)': f\"${revenue_proxy:,.0f}\",\n",
    "    'Platform Market Share': f\"Web: {platform_metrics.loc['web player', 'Market_Share']:.0f}%\",\n",
    "    'Average Session Length': f\"{session_stats['Session_Duration_Minutes'].mean():.1f} min\",\n",
    "    'Overall Skip Rate': f\"{df['is_skip'].mean():.1%}\",\n",
    "    'Peak Usage Hour': f\"{peak_hour}:00\",\n",
    "    'Top Platform (Engagement)': f\"{best_platform}\",\n",
    "    'High Quality Sessions': f\"{quality_distribution.get('High', 0):.1f}%\",\n",
    "    'Content Catalog': f\"{df['spotify_track_uri'].nunique():,} tracks\"\n",
    "}\n",
    "\n",
    "print(\"ðŸ“Š EXECUTIVE SUMMARY\")\n",
    "print(\"=\" * 30)\n",
    "for metric, value in executive_summary.items():\n",
    "    print(f\"{metric:<25}: {value}\")\n",
    "\n",
    "# Save business insights\n",
    "# Create summary dataframes for export\n",
    "business_metrics = pd.DataFrame({\n",
    "    'Metric': list(executive_summary.keys()),\n",
    "    'Value': list(executive_summary.values())\n",
    "})\n",
    "\n",
    "business_metrics.to_csv('../reports/executive_summary.csv', index=False)\n",
    "platform_metrics.to_csv('../reports/platform_analysis.csv')\n",
    "artist_performance.to_csv('../reports/artist_performance.csv')\n",
    "\n",
    "print(\"\\nâœ… Business intelligence reports saved to ../reports/\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
